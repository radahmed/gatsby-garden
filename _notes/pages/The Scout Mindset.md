---
title: The Scout Mindset
tags: [ğŸ“¥ï¸/ğŸ“šï¸, book, 100books]
aliases:  
date created: July 23rd, 2021
---

- Metadata:
	- Type: [[Book]]
	- Author: [[Julia Galef]]
	- Reviewed Date: [[July 23rd, 2021]]
	- Related: 

---

## Notes:

Scout mindset: the motivation to see things as they are, not as you wish they were.

Scout mindset is what allows you to recognize when you're wrong, it strips your thinking off your biases, it's what you use if you want to test your assumptions, if you want to be objective about something, when you would like to change your mind about something, It lets you ask important questions like â€œWas I at fault in that argument?â€ or â€œIs this risk worth it?â€ or â€œHow would I react if someone from the other political party did the same thing?â€

> â€œThe first principle is that you must not fool yourselfâ€”and you are the easiest person to fool." -Richard Feynman

More often than not we try to justify things that we know or we might not know are wrong, human brain hardwired for self-deception: We rationalize away our flaws and mistakes. We indulge in wishful thinking. We cherry-pick evidence that confirms our prejudices and supports our political tribe.

Scout mindset is what you need to stop fooling yourself, to confront the truth, it's not something you've not been doing, in fact you might have found yourself questioning your choices, your mistakes but we don't do it as much as we should be doing.

Knowing _how to reason_ isn't a cure-all, just like knowing the importance of exercising isn't going to make you healhty. Unless you acknowledge your biases and fallacies in your thinking and put in conscious effort to fight them off.

> Our judgment isnâ€™t limited by knowledge nearly as much as itâ€™s limited by attitude. - Julia Galef

- The Truth isn't in conflict with your other goals.
- Learn tools that make it easier to see clearly.
  - eg: outsider test, the selective skeptic test, the conformity test, introspection techniques
- Appreciating the emotional rewards of scout mindset.

Scout mindset is rewarding in the sense that it allows us to be open minded no matter the situation we're in, there's equanimity that results from understanding risk and coming to terms with the odds youâ€™re facing. And thereâ€™s a refreshing lightness in the feeling of being free to explore ideas and follow the evidence wherever it leads, unconstrained by what youâ€™re â€œsupposed toâ€ think.

It's a different way of being, one thatâ€™s rooted in an appetite for truth, and one thatâ€™s both useful and fulfilling

To be willing to consider other interpretationsâ€”to even believe that there could be other reasonable interpretations besides your ownâ€”requires scout mindset.

---

The opposite of [[directionally-motivated-reasoning]] is [[accuracy-motivated-reasoning]]

---

## Quick Overview of Soldier vs Scout Mindset

| Soldier Mindset                                                                                                    | Scout Mindset                                                                                               |
| ------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------- |
| Reasoning is like defensive combat.                                                                                | Reasoning is like mapmaking.                                                                                |
| Decide what to believe by asking either â€œCan I believe this?â€ or â€œMust I believe this?â€ depending on your motives. | Decide what to believe by asking, â€œIs this true?â€                                                           |
| Finding out youâ€™re wrong means suffering a defeat.                                                                 | Finding out youâ€™re wrong means revising your map.                                                           |
| Seek out evidence to fortify and defend your beliefs.                                                              | Seek out evidence that will make your map more accurate.                                                    |
| Related concepts: Directionally motivated reasoning, rationalizing, denial, self-deception, wishful thinking       | Related concepts: Accuracy motivated reasoning, truth-seeking, discovery, objectivity, intellectual honesty |

---

While we're not all perfect scouts, we should always try, in all contexts of our lives to stay in the scout mindset.

To let go of the [[soldier-mindset]] we need to ask why it was there in the first place.

## We make unconscious trade-offs

We trade off between judgment and belonging. If you live in a tight-knit community, it might be easier to fit in if you use soldier mindset to fight off any doubts you have about your communityâ€™s core beliefs and values. On the other hand, if you do allow yourself to entertain those doubts, you might realize youâ€™re better off rejecting your communityâ€™s views on morality, religion, or gender roles, and deciding to live a less traditional life.

- We trade off between judgment and persuasion.
- We trade off between judgment and morale.
  - When you come up with a plan, focusing only on its positives (â€œThis is such a great idea!â€) can help you work up enthusiasm and motivation to carry it out. On the other hand, if you scrutinize your plan for flaws (â€œWhat are the downsides? How might this fail?â€), youâ€™re more likely to notice if thereâ€™s a better plan you should switch to instead.

We make these trade-offs, and many more, all the time, usually without even realizing weâ€™re doing so. After all, the whole point of self-deception is that itâ€™s occurring beneath our conscious awareness. If you were to find yourself thinking, explicitly, â€œShould I admit to myself that I screwed up?â€ the issue would already be moot. So itâ€™s left up to our unconscious minds to choose, on a case-by-case basis, which goals to prioritize. Sometimes we choose soldier mindset, furthering our emotional or social goals at the expense of accuracy. Sometimes we choose scout mindset, seeking out the truth even if it turns out not to be what we were hoping for.

And sometimes our unconscious minds attempt to have it both ways, our desire to protect our self-esteem and happiness can be in competition with our desire to learn about problems we're facing(so that We can fix them)

## Are we rationally irrational?

The hypothesis that the human mind evolved the ability to make these trade-offs well is called the rational irrationality hypothesis, if the name sounds like a paradox, thatâ€™s because itâ€™s using two different senses of the word rational:[[espistemic-rationality]] means holding beliefs that are well justified, while [[instrumental-rationality]] means acting effectively to achieve your goals.

Being rationally irrational, therefore, would mean that weâ€™re good at unconsciously choosing just enough epistemic irrationality to achieve our social and emotional goals, without impairing our judgment too much. A rationally irrational person would deny problems only when the comfort of denial is sufficiently high and their chance of fixing the problem is sufficiently low.

So, are we rationally irrational?

weâ€™re far from rationally irrational. There are several major biases in our decision-making, several ways in which we systematically misjudge the costs and benefits of truth.
Our biases cause us to overvalue soldier mindset, choosing it more often than we should, and undervalue scout mindset, choosing it less often than we should.

## We overvalue the immediate rewards of solider mindset

One of the most frustrating aspects of being human is our knack for undermining our own goals.

The source of this self-sabotage is present bias, a feature of our intuitive decision-making in which we care too much about short-term consequences and too little about long-term consequences. In other words, weâ€™re impatient, and we get more impatient as the potential rewards grow closer.

When you contemplate a gym membership, the trade-off seems well worth it, in theory. Spend a few hours per week exercising, in exchange for looking and feeling a lot better? Sign me up! But on any given morning, when youâ€™re faced with the choice between â€œTurn off my alarm and sink blissfully back into sleepâ€ or â€œHead to the gym and make an imperceptible amount of progress toward my fitness goals,â€ itâ€™s a much tougher call. The rewards of choosing to sleep in are immediate; the rewards of choosing to exercise are diffuse and delayed. What difference will one exercise session make to your long-term fitness goals, anyway?

Itâ€™s widely known that present bias shapes our choices about how to act. Whatâ€™s much less appreciated is that it also shapes our choices about how to think. Just like sleeping in, breaking your diet, or procrastinating on your work, we reap the rewards of thinking in soldier mindset right away, while the costs donâ€™t come due until later. If youâ€™re worried about a mistake you made and you convince yourself that â€œIt wasnâ€™t my fault,â€ youâ€™re rewarded with a hit of instant emotional relief. The cost is that you miss out on learning from your mistake, which means youâ€™re less able to prevent it from happening again. But that wonâ€™t affect you until some unknown point in the future.

Being overly optimistic about your chance of success gives you a burst of motivation right away. But those motivational benefits dwindle over time, or even backfire, when success takes longer than you predicted. As Francis Bacon said, â€œHope is a good breakfast, but a bad supper.â€

## We underestimate the value of building scout habits

When you wake up in the morning and head to the gym, the benefit of that choice isnâ€™t just in the calories you burn or the muscle tone you develop that day. The benefit also lies in the fact that youâ€™re reinforcing valuable skills and habits. That includes the habit of going to the gym, obviously, but also the broader skill of doing hard things, and the broader habit of following through on your promises to yourself.

Any single day, on its own, doesnâ€™t make much difference to your overall habits and skills. â€œI can just go tomorrow,â€ you think as you turn off the alarm. Which is trueâ€”but of course, youâ€™ll think the same thing tomorrow.

Even when youâ€™re reasoning about something like foreign politics that doesnâ€™t impact your life directly, the way you think still impacts you indirectly, because youâ€™re reinforcing general habits of thought. Every time you say, â€œOh, thatâ€™s a good point, I hadnâ€™t thought of that,â€ it gets a little bit easier for you to acknowledge good points in general. Every time you opt to check a fact before citing it, you become a little bit more likely to remember to check your facts in general. Every time youâ€™re willing to say, â€œI was wrong,â€ it gets a little bit easier to be wrong in general.

Itâ€™s hard for the â€œIncrementally improve my thinking habitsâ€ benefit to compete with the vivid and immediate rewards of soldier mindset.

## we underestimate the ripple effects of self-deception

When you tell a lie, itâ€™s hard to predict exactly what youâ€™ve committed your future self to.

Just like the lies we tell others, the lies we tell ourselves have ripple effects. Suppose you tend to rationalize away your own mistakes, and consequently you see yourself as more perfect than you really are. This has a ripple effect on your views of other people: Now, when your friends and family screw up, you might not be very sympathetic. After all, you never make such mistakes. Why canâ€™t they just be better? Itâ€™s not that hard.

Or suppose that for the sake of your self-esteem, you view yourself through rose-colored glasses, judging yourself to be more charming, interesting, and impressive than you actually appear to other people. Hereâ€™s one possible ripple effect: How do you explain the fact that women donâ€™t seem to be interested in dating you, given what a great catch you are? Well, maybe theyâ€™re all shallow.

Itâ€™s hard to know exactly how the ripple effect of a particular act of self-deception will hurt you in the future, or if it will at all. Perhaps in many instances the harm is negligible. But the fact that the harm is delayed and unpredictable should ring an alarm bell. This is exactly the kind of cost we tend to neglect when weâ€™re intuitively weighing costs and benefits. Ripple effects are yet more reason to suspect that we underestimate the cost of deceiving ourselvesâ€”and are therefore choosing soldier mindset too often and scout mindset not often enough.

## We overestimate social costs

Another way in which our intuition about costs and benefits is skewedâ€”we overestimate the importance of how we come across to other people. Social costs like looking weird or making a fool out of ourselves feel a lot more significant than they actually are. In reality, other people arenâ€™t thinking about you nearly as much as you intuitively think they are, and their opinions of you donâ€™t have nearly as much impact on your life as it feels like they do.

As a result, we end up making tragic trade-offs, sacrificing a lot of potential happiness to avoid relatively small social costs.

When we allow ourselves to reflect on a social cost weâ€™ve been avoiding (or when someone else prompts us to reflect on it, we often realize, â€œHey, this isnâ€™t such a big deal after all. I can decide to take on a little more responsibility at work, and itâ€™ll be fine. No oneâ€™s going to hate me for that.â€ But when we leave the decision up to our instincts, even a hint of potential social risk prompts a reflexive â€œAvoid at all costs!â€ reaction.

---

Weâ€™re overly tempted by immediate payoffs, even when they come at a steep cost later on. We underestimate the cumulative harm of false beliefs, and the cumulative benefit of practicing scout habits. We overestimate how much other people judge us, and how much impact their judgments have on our lives. As a result of all these tendencies, we end up being far too willing to sacrifice our ability to see clearly in exchange for short-term emotional and social rewards. That doesnâ€™t mean scout mindset is always the better choiceâ€”but it does mean we have a bias in favor of the soldier, even when the scout is a better choice.

## An accurate map is more useful now

We have far more options now.

Whether your choices make your life better or worse depends on your judgment, and your judgment depends on your mindset.

Living in the modern world also means we have many more opportunities to fix things we donâ€™t like about our lives. If youâ€™re bad at something, you can take classes, read a For Dummies book, watch a YouTube tutorial, get a tutor, or hire someone to do it for you. If youâ€™re chafing under the constrictive social mores of your town, you can find kindred spirits online or move to a big city. If your family is abusive, you can cut ties with them.

Not all of these solutions are equally effective for everyone, and not all of them are worth the effort or cost. Deciding which solutions are worth trying is a matter of judgment. Deciding which problems in your life are worth trying to solve at all, versus simply learning to live with, is a matter of judgment, too.

This abundance of opportunity makes scout mindset far more useful than it would have been for our ancestors. After all, whatâ€™s the point of admitting your problems exist if you canâ€™t fix them? Whatâ€™s the point of noticing your disagreements with your community if you canâ€™t leave? Having an accurate map doesnâ€™t help you very much when youâ€™re allowed to travel only one path.

So if our instincts undervalue truth, thatâ€™s not surprisingâ€”our instincts evolved in a different world, one better suited to the soldier. Increasingly, our world is becoming one that rewards the ability to see clearly, especially in the long run; a world in which your happiness isnâ€™t nearly as dependent on your ability to accommodate yourself to whatever life, skills, and social groups you happened to be born into.

More and more, itâ€™s a scoutâ€™s world now.

## Feeling objective doesn't make you a scout

We think of ourselves as objective because we feel objective. We scrutinize our own logic and it seems sound. We donâ€™t detect any signs of bias in ourselves. We feel unemotional, dispassionate.
But the fact that you feel calm doesnâ€™t mean youâ€™re being fair.

being able to explain a position â€œrationally,â€ - by which people usually mean that they can make a compelling argument in favor of their positionâ€”doesnâ€™t mean the position is fair. Of course your argument seems compelling to you; everyoneâ€™s argument seems compelling to them. Thatâ€™s how motivated reasoning works.

In fact, viewing yourself as rational can backfire. The more objective you think you are, the more you trust your own intuitions and opinions as accurate representations of reality, and the less inclined you are to question them.

When you start from the premise that youâ€™re an objective thinker, you lend your conclusions an air of unimpeachability they usually donâ€™t deserve.

## Being smart and knowledgeable doesn't make you a scout

when you ask people for their opinions on other ideologically charged scientific issues: Should the government fund stem cell research? How did the universe begin? Did humans evolve from lower animal species? On all these questions, the people with the highest levels of scientific intelligence were also the most politically polarized in their opinions.

Truth doesn't always lie in the center. If anyone claims so, that would be false balance.

On any particular issue, the truth may lie close to the far left or the far right or anywhere else. The point is simply that as people become better informed, they should start to converge on the truth, wherever it happens to be. Instead, we see the opposite patternâ€”as people get better informed, they diverge.

This is a crucially important result, because being smart and being knowledgeable on a particular topic are two more things that give us a false sense of security in our own reasoning. A high IQ and an advanced degree might give you an advantage in ideologically neutral domains like solving math problems or figuring out where to invest your money. But they wonâ€™t protect you from bias on ideologically charged questions.

Intelligence and knowledge are just tools. You can use those tools to help you see the world clearly, if thatâ€™s what youâ€™re motivated to do. Or you can use them to defend a particular viewpoint, if youâ€™re motivated to do that instead. But thereâ€™s nothing inherent to the tools that makes you a scout.

## Actually practicing scout mindset makes you a scout

Itâ€™s easy to think, â€œOf course I change my mind in response to evidence,â€ or â€œOf course I apply my principles consistently,â€ or â€œOf course Iâ€™m fair-minded,â€ whether or not those things are true. The test of scout mindset isnâ€™t whether you see yourself as the kind of person who does these things. Itâ€™s whether you can point to concrete cases in which you did, in fact, do these things.

Feeling reasonable, being smart and knowledgeable, being aware of motivated reasoningâ€”all these things seem like they should be indicators of scout mindset, yet they have surprisingly little to do with it. The only real sign of a scout is whether you act like one.

Five signs of scout mindset, behavioral cues that someone cares about truth and will seek it out even when theyâ€™re not forced to, and even when the truth isnâ€™t favorable to them.

- ### 1. Do you tell other people when you realize they were right?

Technically, scout mindset only requires you to be able to acknowledge to yourself that you were wrong, not to other people. Still, a willingness to say â€œI was wrongâ€ to someone else is a strong sign of a person who prizes the truth over their own ego. Can you think of cases in which youâ€™ve done the same?

- ### 2. How do you react to personal criticism?

Itâ€™s a lot easier to say you welcome criticism than it is to actually welcome it. But in so many domains, getting honest feedback is essential to improvement.

To gauge your comfort with criticism, itâ€™s not enough just to ask yourself, â€œAm I open to criticism?â€ Instead, examine your track record. Are there examples of criticism youâ€™ve acted upon? Have you rewarded a critic (for example, by promoting him)? Do you go out of your way to make it easier for other people to criticize you?

- ### 3. Do you ever prove yourself wrong?

â€œWe all identified with something because it sounded like our reality,â€

Can you think of any examples in which you voluntarily proved yourself wrong? Perhaps you were about to voice an opinion online, but decided to search for counterarguments first, and ended up finding them compelling. Or perhaps at work you were advocating for a new strategy, but changed your mind after you ran the numbers more carefully and realized it wouldnâ€™t be feasible.

- ### 4. Do you take precautions to avoid fooling yourself?

Do you try to avoid biasing the information you get? For example, when you ask your friend to weigh in on a fight you had with your partner, do you describe the disagreement without revealing which side you were on, so as to avoid influencing your friendâ€™s answer? When you launch a new project at work, do you decide ahead of time what will count as a success and what will count as a failure, so youâ€™re not tempted to move the goalposts later?

- ### 5. Do you have any good critics?

Itâ€™s tempting to view your critics as mean-spirited, ill-informed, or unreasonable. And itâ€™s likely that some of them are. But itâ€™s unlikely that all of them are. Can you name people who are critical of your beliefs, profession, or life choices who you consider thoughtful, even if you believe theyâ€™re wrong? Or can you at least name reasons why someone might disagree with you that you would consider reasonable (even if you donâ€™t happen to know of specific people who hold those views)?

---

Being able to name reasonable critics, being willing to say â€œThe other side has a point this time,â€ being willing to acknowledge when you were wrongâ€”itâ€™s things like these that distinguish people who actually care about truth from people who only think they do.

But the biggest sign of scout mindset may be this: Can you point to occasions in which you were in soldier mindset? If that sounds backward, remember that motivated reasoning is our natural state. Itâ€™s universal, hardwired into our brains. So if you never notice yourself doing it, whatâ€™s more likelyâ€”that you happen to be wired differently from the rest of humanity or that youâ€™re simply not as self-aware as you could be?

Learning to spot your own biases, in the moment, is no easy feat. But itâ€™s not impossible, if you have the right tools. Thatâ€™s what the next two chapters are about.

---

## Noticing biases

If you get sued and you win the case, should the person who sued you pay your legal costs? If youâ€™re like most people (85 percent, in one study1), your answer is yes. After all, if youâ€™re falsely accused of something, why should you be out thousands of dollars in lawyersâ€™ fees? That wouldnâ€™t be fair.

However, when the question in that study was slightly rewordedâ€”â€œIf you sue someone and you lose the case, should you pay his costs?â€â€”only 44 percent of people said yes. Imagining yourself in the role of the person who sued and lost brings to mind alternate arguments. For example, you might have lost simply because the other side is wealthy and can afford better lawyers. Itâ€™s not fair to discourage victims from suing just because they canâ€™t afford to lose, right?

Both the arguments for and against the â€œloser paysâ€ policy have at least some merit. But which one comes to mind will depend on whether youâ€™re the plaintiff or the defendantâ€”and it will likely never occur to you that you could have thought of an opposing argument if you had been on the other side of the case.

## A thought experiment is a peak into the counterfactual world

You canâ€™t detect motivated reasoning in yourself just by scrutinizing your reasoning and concluding that it makes sense. You have to compare your reasoning to the way you would have reasoned in a counterfactual world, a world in which your motivations were differentâ€”would you judge that politicianâ€™s actions differently if he was in the opposite party? Would you evaluate that advice differently if your friend had offered it instead of your spouse? Would you consider that studyâ€™s methodology sound if its conclusions supported your side?

You canâ€™t literally visit the counterfactual world. But you can do the next best thingâ€”peek into it virtually, with a thought experiment.

lets explore five different types of thought experiments: the double standard test, the outsider test, the conformity test, the selective skeptic test, and the status quo bias test.

Thought experiments only work if you actually do them. So donâ€™t simply formulate a verbal question for yourself. Conjure up the counterfactual world, place yourself in it, and observe your reaction.

---

## Common thought experiments

| Name                       | Definition                                                                                                      |
| -------------------------- | --------------------------------------------------------------------------------------------------------------- |
| The Outsider Test          | How would you evaluate this situation if it wasnâ€™t your situation?                                              |
| The Conformity Test        | If other people no longer held this view, would you still hold it?                                              |
| The Selective Skeptic Test | If this evidence supported the other side, how credible would you judge it to be?                               |
| The Status Quo Bias Test   | If your current situation was not the status quo, would you actively choose it?                                 |
| The Double Standard Test   | Are you judging one person (or group) by a different standard than you would use for another person (or group)? |

---

What thought experiments do is simply reveal that your reasoning changes as your motivations change. That the principles youâ€™re inclined to invoke or the objections that spring to your mind depend on your motives: the motive to defend your image or your in-groupâ€™s status; the motive to advocate for a self-serving policy; fear of change or rejection.

Catching your brain in the act of motivated reasoningâ€”noticing when an experimentâ€™s previously invisible flaws jump out at you, or noticing that your preferences change as you switch around supposedly irrelevant details of a scenarioâ€”breaks down the illusion that your initial judgment is the objective truth. It convinces you, viscerally, that your reasoning is contingent; that your initial judgments are a starting point for exploration, not an end point.

# How Sure Are You?

## We like feeling certain

Overconfidence, when confidence that one is right outstrips the actual accuracy. Doctors

The certainty we express is partly just for simplicityâ€™s sake. Conversation would be unwieldy if we had to stop and assign a probability to every statement we made. But even when someone does prompt us to stop and reflect on our level of confidence, we often claim to be completely certain.

Even professionals are frequently certain and wrong in their area of expertise. For example, many studies have found that doctors routinely overestimate their ability to diagnose patients.

If we tend to be overly certain about our knowledge, thatâ€™s even more the case when it comes to our opinions.

Not all overconfidence is due to motivated reasoning. Sometimes we simply donâ€™t realize how complicated a topic is, so we overestimate how easy it is to get the right answer. But a large portion of overconfidence stems from a desire to feel certain. Certainty is simple. Certainty is comfortable. Certainty makes us feel smart and competent.

Your strength as a scout is in your ability to resist that temptation, to push past your initial judgment, and to think in shades of gray instead of black and white. To distinguish the feeling of â€œ95% sureâ€ from â€œ75% sureâ€ from â€œ55% sure.â€

## Quantifying your uncertainty

A scout treats their degree of certainty as a prediction of their likelihood of being right.

Imagine sorting all of your beliefs into buckets based on how sure you are that youâ€™re right about each one. This would include quotidian predictions (â€œI will enjoy this restaurantâ€), beliefs about your life (â€œMy partner is faithful to meâ€), beliefs about how the world works (â€œSmoking causes cancerâ€), core premises (â€œMagic isnâ€™t realâ€), and so on. Putting a belief into the â€œ70% sureâ€ bucket is like saying, â€œThis is the kind of thing I expect to get right roughly 70 percent of the time.â€

What youâ€™re implicitly aiming for when you tag your beliefs with various confidence levels is perfect calibration. That means your â€œ50% sureâ€ claims are in fact correct 50 percent of the time, your â€œ60% sureâ€ claims are correct 60 percent of the time, your â€œ70% sureâ€ claims are correct 70 percent of the time, and so on.

Perfect calibration is an abstract ideal, not something thatâ€™s possible to achieve in reality. Still, itâ€™s a useful benchmark against which to compare yourself.

## A bet can reveal how sure you really are

Imagine youâ€™re talking with a friend whoâ€™s struggling to get her new catering business off the ground. You reassure her, â€œYouâ€™re amazing at this stuff! The only reason business is slow is because youâ€™re just starting out. Everyone has trouble getting clients at first!â€

She replies, â€œThanks! Iâ€™m so glad you feel that way. Could you recommend me to your coworkers?â€

Suddenly you feel hesitant. You recall her telling you about backing out of a job at the last minuteÂ .Â .Â . and you realize that youâ€™ve never actually tasted her cookingÂ .Â .Â . and you canâ€™t help but ask yourself, â€œHow sure am I, really, that sheâ€™ll do a decent job?â€

When you were reassuring your friend a moment earlier, you werenâ€™t lying. You just werenâ€™t thinking about what you actually believed, because it didnâ€™t seem to matter. But once there are actual stakes, and your reputation could take a hit if you guess wrong about your friendâ€™s catering skill, your brain switches over from the goal â€œbe supportiveâ€ to the goal â€œactually try to get the right answer.â€

Imagine that a company sells toothpaste. The press secretary might assert confidently, â€œOur toothpaste whitens teeth better than any other brand on the market.â€ But suppose the board is approached by a dental school professor who says, â€œIâ€™d like to do a study. Iâ€™ll assign groups of people to use one of the leading brands of toothpaste, without telling them which brand it is, and then Iâ€™ll measure how much whiter their teeth are. Iâ€™ll publish whatever results I get.â€

If the board was truly confident that their toothpaste worked best, they would say, â€œGreatâ€”a chance to prove to the public that weâ€™re the best!â€ But despite the press secretaryâ€™s assurances, the board might decide that theyâ€™re not confident enough they would win such a contest, and itâ€™s not worth risking the embarrassment of losing.

The press secretary isnâ€™t thinking about whatâ€™s true. Heâ€™s thinking about what he can get away with saying, what will present the company in the best light while still being at least sort of plausible. But the board is incentivized to form their best guess about the truth, because the company will thrive if they guess right and suffer if theyâ€™re wrong. The press secretary makes claims; the board makes bets.

The word bet might conjure up horse races and blackjack tables, but its meaning is far more general. A bet is any decision in which you stand to gain or lose something of value, based on the outcome. That could include money, health, timeâ€”or reputation, as in the case of your catering friend who wants your endorsement. So when youâ€™re thinking about how sure you are, your answer will be more honest if you switch from thinking in terms of â€œWhat can I get away with claiming to myself?â€ to â€œHow would I bet, if there was something at stake?â€

A tip when youâ€™re imagining betting on your beliefs: You may need to get more concrete about what you believe by coming up with a hypothetical test that could be performed to prove you right or wrong. For example, if you believe â€œOur computer servers are highly secure,â€ a hypothetical test might be something like this: Suppose you were to hire a hacker to try to break in to your systems. If they succeed, you lose one monthâ€™s salary. How confident do you feel that you would win that bet?

## The equivalent bet test

The examples of bets in the previous section are meant to generate a qualitative sense of your confidence in a belief. Do you feel happy to take the bet, without hesitating? Do you feel a little bit of doubt? Do you feel truly torn? Your hesitation, or lack thereof, is a proxy for your degree of confidence that your belief is true.

Contemplating a bet can also be used to pin down how sure you are quantitatively, helping you put a number on your degree of confidence.

I can bet on self-driving cars, and get $10,000 if theyâ€™re on the market in a year. Alternately, I can take the â€œball betâ€: Iâ€™m given a box containing four balls, one of which is gray. I reach in and pull out one ball, without lookingâ€”if itâ€™s gray, I win $10,000.

Which gamble would I rather take? I hesitate for a moment, but I feel happier with the ball bet. Since the probability of winning the ball bet is 1 in 4 (or 25 percent), the fact that I feel more confident in the ball bet implies that Iâ€™m less than 25 percent confident in self-driving cars making it to the market in a year.

Letâ€™s try decreasing the odds of winning the ball bet. Suppose the box contains sixteen balls, only one of which is gray. Now which do I prefer: betting on drawing the gray ball or betting on self-driving cars in a year?

This time, I notice that I prefer my chances on self-driving cars. After all, sometimes technological progress surprises us. Maybe one of the companies working on self-driving technology is actually farther along than theyâ€™ve been letting on. It seems unlikely, but Iâ€™d still rather gamble on that than on being lucky enough to draw the gray ball. And since the probability of drawing a gray ball is 1 in 16 (or about 6 percent), the fact that I would prefer to bet on self-driving cars implies that I am more than 6 percent confident that self-driving cars will make it to the market in a year.

Okay, letâ€™s adjust the chances of winning the ball bet back upward a little bit, to one in nine. Now which do I prefer?

Hmm. Iâ€™m really torn. Neither seems like a clearly better bet. The bets feel equivalent to meâ€”and since we know that the probability of winning the ball bet is 1 in 9 (or about 11 percent), that implies that I have roughly 11 percent confidence in self-driving cars coming out within a year. I still donâ€™t think the â€œSelf-driving cars will be on the market in a yearâ€ forecast is likely to come true, but Iâ€™ve gone from a glib â€œThatâ€™s crazyâ€ to a more honest best guess.

being able to tell the difference between the feeling of making a claim and the feeling of actually trying to guess whatâ€™s true. Making a claim feels like your press secretary is speaking. It feels pat; neat and tidy. Sometimes hurried, as if youâ€™re trying to put something past yourself. The mental motion is declaring, proclaiming, insisting, or perhaps scoffing.

---

# How Sure Are You?

In a scene in the 2016 movie Star Trek Beyond, a spaceship careens across the sky.1 Itâ€™s being piloted by Captain Kirk, who is hot on the tail of three enemy ships headed straight for the center of a city, where they intend to detonate a superweapon. Kirkâ€™s right-hand man, Commander Spock, yells to him: â€œCaptain, intercepting all three ships is an impossibility!â€

An impossibility. The words sound so authoritative, so definitive. And yet less than sixty seconds later, Kirk has figured out how to maneuver in front of the enemy ships, stopping them with his own shipâ€™s hull before they can reach their destination.

If youâ€™ve watched much Star Trek before, this wonâ€™t surprise you. Spock doesnâ€™t have a great track record when it comes to making accurate predictions. â€œThereâ€™s only a very slight chance this will work,â€ Spock warns Kirk in one episode of the original TV show, right before their plan works.2 The odds of survival are â€œless than seven thousand to one,â€ Spock tells Kirk in another episode, shortly before they escape unharmed.3 The chance of finding survivors is â€œabsolutely none,â€ Spock declares in yet another episode, right before they discover a large colony of survivors.4

## We like feeling certain

Spock is overconfident, meaning that his confidence that heâ€™s right outstrips his actual accuracy. In that respect, Spock isnâ€™t all that different from most of us (except that he makes a much bigger deal about how his predictions are objective and â€œlogical,â€ which is why Iâ€™ve chosen to make an example of him). We very often speak as if thereâ€™s no chance we could be mistakenâ€”â€œThereâ€™s no way he can make that shot from that distance!â€ or â€œIâ€™ll definitely have that done by Fridayâ€â€”and yet we turn out to be wrong nonetheless.

To be fair, the certainty we express is partly just for simplicityâ€™s sake. Conversation would be unwieldy if we had to stop and assign a probability to every statement we made. But even when someone does prompt us to stop and reflect on our level of confidence, we often claim to be completely certain. Youâ€™ll notice this if you search online for phrases like, â€œHow certain are you thatâ€ or â€œHow confident are you that.â€ Here are a few examples I pulled from discussions on Quora, Yahoo! Answers, Reddit, and other forums:

As a percentage, how certain are you that intelligent life exists outside of Earth? â€œI am 100% certain thereâ€™s other intelligent life.â€5

How confident are you that you are going to hit your 2017 sales goals? â€œIâ€™m 100% confident.â€6

Atheists, how confident are you that you wonâ€™t convert to a religion like Christianity on your deathbed? â€œ100% confident.â€7

Even professionals are frequently certain and wrong in their area of expertise. For example, many studies have found that doctors routinely overestimate their ability to diagnose patients. One study examined the autopsy results for patients who had been given diagnoses with â€œcomplete certainty,â€ and found that in 40 percent of those cases, the diagnosis was incorrect.8

If we tend to be overly certain about our knowledge, thatâ€™s even more the case when it comes to our opinions. We say things like, â€œThere is no question that America needs a living wage,â€ or â€œItâ€™s obvious that the internet has wrecked our attention spans,â€ or â€œOf course that bill would be a disaster.â€

Not all overconfidence is due to motivated reasoning. Sometimes we simply donâ€™t realize how complicated a topic is, so we overestimate how easy it is to get the right answer. But a large portion of overconfidence stems from a desire to feel certain. Certainty is simple. Certainty is comfortable. Certainty makes us feel smart and competent.

Your strength as a scout is in your ability to resist that temptation, to push past your initial judgment, and to think in shades of gray instead of black and white. To distinguish the feeling of â€œ95% sureâ€ from â€œ75% sureâ€ from â€œ55% sure.â€ Thatâ€™s what weâ€™ll learn to do in this chapter.

But first, letâ€™s back upâ€”what does it even mean to put a number on your degree of belief?

## Quantifying your uncertainty

Typically, when people think about how sure they are, they ask themselves something like: â€œDo I actively feel any doubt?â€ If the answer is no, as it often is, they declare themselves to be â€œ100% certain.â€

Thatâ€™s an understandable way to think about certainty, but itâ€™s not the way a scout thinks about it. A scout treats their degree of certainty as a prediction of their likelihood of being right. Imagine sorting all of your beliefs into buckets based on how sure you are that youâ€™re right about each one. This would include quotidian predictions (â€œI will enjoy this restaurantâ€), beliefs about your life (â€œMy partner is faithful to meâ€), beliefs about how the world works (â€œSmoking causes cancerâ€), core premises (â€œMagic isnâ€™t realâ€), and so on. Putting a belief into the â€œ70% sureâ€ bucket is like saying, â€œThis is the kind of thing I expect to get right roughly 70 percent of the time.â€

What youâ€™re implicitly aiming for when you tag your beliefs with various confidence levels is perfect calibration. That means your â€œ50% sureâ€ claims are in fact correct 50 percent of the time, your â€œ60% sureâ€ claims are correct 60 percent of the time, your â€œ70% sureâ€ claims are correct 70 percent of the time, and so on.

## Perfect calibration

Perfect calibration is an abstract ideal, not something thatâ€™s possible to achieve in reality. Still, itâ€™s a useful benchmark against which to compare yourself. To get a hang of the concept, letâ€™s continue picking on Spock and see how his calibration measures up against perfection.

## A bet can reveal how sure you really are

Imagine youâ€™re talking with a friend whoâ€™s struggling to get her new catering business off the ground. You reassure her, â€œYouâ€™re amazing at this stuff! The only reason business is slow is because youâ€™re just starting out. Everyone has trouble getting clients at first!â€

She replies, â€œThanks! Iâ€™m so glad you feel that way. Could you recommend me to your coworkers?â€

Suddenly you feel hesitant. You recall her telling you about backing out of a job at the last minuteÂ .Â .Â . and you realize that youâ€™ve never actually tasted her cookingÂ .Â .Â . and you canâ€™t help but ask yourself, â€œHow sure am I, really, that sheâ€™ll do a decent job?â€

When you were reassuring your friend a moment earlier, you werenâ€™t lying. You just werenâ€™t thinking about what you actually believed, because it didnâ€™t seem to matter. But once there are actual stakes, and your reputation could take a hit if you guess wrong about your friendâ€™s catering skill, your brain switches over from the goal â€œbe supportiveâ€ to the goal â€œactually try to get the right answer.â€

Evolutionary psychologist Robert Kurzban has an analogy for these two modes.11 In a company, thereâ€™s a board of directors whose role is to make the crucial decisions for the companyâ€”how to spend its budget, which risks to take, when to change strategies, and so on. Then thereâ€™s a press secretary whose role is to give statements about the companyâ€™s values, its mission, and the reasoning behind its decisions.

If a competitor starts gaining market share, the companyâ€™s press secretary might assure the public, â€œWeâ€™re not worried. Our brand has been Americaâ€™s favorite for thirty years, and thatâ€™s not going to change.â€ However, if you were to sit in on a board meeting, you might find that behind the scenes, the board is taking the risk seriously and looking for ways to cut costs.

Imagine that the company sells toothpaste. The press secretary might assert confidently, â€œOur toothpaste whitens teeth better than any other brand on the market.â€ But suppose the board is approached by a dental school professor who says, â€œIâ€™d like to do a study. Iâ€™ll assign groups of people to use one of the leading brands of toothpaste, without telling them which brand it is, and then Iâ€™ll measure how much whiter their teeth are. Iâ€™ll publish whatever results I get.â€

If the board was truly confident that their toothpaste worked best, they would say, â€œGreatâ€”a chance to prove to the public that weâ€™re the best!â€ But despite the press secretaryâ€™s assurances, the board might decide that theyâ€™re not confident enough they would win such a contest, and itâ€™s not worth risking the embarrassment of losing.

The press secretary isnâ€™t thinking about whatâ€™s true. Heâ€™s thinking about what he can get away with saying, what will present the company in the best light while still being at least sort of plausible. But the board is incentivized to form their best guess about the truth, because the company will thrive if they guess right and suffer if theyâ€™re wrong. The press secretary makes claims; the board makes bets.

The word bet might conjure up horse races and blackjack tables, but its meaning is far more general. A bet is any decision in which you stand to gain or lose something of value, based on the outcome. That could include money, health, timeâ€”or reputation, as in the case of your catering friend who wants your endorsement. So when youâ€™re thinking about how sure you are, your answer will be more honest if you switch from thinking in terms of â€œWhat can I get away with claiming to myself?â€ to â€œHow would I bet, if there was something at stake?â€

Sometimes a project Iâ€™m working on seems hopeless. For exampleâ€”just to pull a random, hypothetical situation out of thin air: â€œThe book Iâ€™m writing is terrible and I should give up.â€ But how sure am I that Iâ€™m not just in a temporary funk? Iâ€™m 100% sure, my press secretary insistsâ€”but letâ€™s ignore him, and pose a question to the board instead: â€œSuppose you would win $1,000 for correctly guessing whether you will still feel this way about your book a week from now. How would you bet?â€

Now that thereâ€™s money on the line, I hesitate. I recall that I have felt pessimistic about my book, or some other project, many times in the past, and the dark cloud usually goes away in a day or two. It feels like a better bet to pick â€œYes, I probably will feel better.â€ Going through that exercise doesnâ€™t magically get rid of my bad mood, but it does take the edge off it. Itâ€™s useful to have proven to myself that I wouldnâ€™t be willing to bet on this mood lasting, even though it feels like it will last forever.

A tip when youâ€™re imagining betting on your beliefs: You may need to get more concrete about what you believe by coming up with a hypothetical test that could be performed to prove you right or wrong. For example, if you believe â€œOur computer servers are highly secure,â€ a hypothetical test might be something like this: Suppose you were to hire a hacker to try to break in to your systems. If they succeed, you lose one monthâ€™s salary. How confident do you feel that you would win that bet?

If you believe â€œI was being reasonable in that fight with my partner, and he was being unreasonable,â€ a hypothetical test might go something like this: Suppose another person, an objective third party, is given all of the relevant details about the fight, and is asked to judge which of you two is being more reasonable. If he judges in your favor, you win $1,000; if not, you lose $1,000. How confident do you feel that you would win that bet?

THE EQUIVALENT BET TEST
The examples of bets in the previous section are meant to generate a qualitative sense of your confidence in a belief. Do you feel happy to take the bet, without hesitating? Do you feel a little bit of doubt? Do you feel truly torn? Your hesitation, or lack thereof, is a proxy for your degree of confidence that your belief is true.

Contemplating a bet can also be used to pin down how sure you are quantitatively, helping you put a number on your degree of confidence. Sometimes I hear an ambitious technological forecast like, â€œSelf-driving cars will be on the market within the year!â€ My first reaction is often to scoff, â€œWell, thatâ€™s crazy.â€ But how sure am I that the forecast is wrong?

To answer that question, I imagine facing a choice between two possible bets. I use a technique I adapted from decision-making expert Douglas Hubbard called an â€œequivalent bet test.â€12 Hereâ€™s how it works in this case: I can bet on self-driving cars, and get $10,000 if theyâ€™re on the market in a year. Alternately, I can take the â€œball betâ€: Iâ€™m given a box containing four balls, one of which is gray. I reach in and pull out one ball, without lookingâ€”if itâ€™s gray, I win $10,000.\*

Ball bet (1 in 4 chance of winning):

Bet on self-driving cars

Draw from a box with four balls, one of which is gray. If I draw the gray ball, I get $10,000.

If fully self-driving cars are available for purchase in a year, I get $10,000.

Which gamble would I rather take? I hesitate for a moment, but I feel happier with the ball bet. Since the probability of winning the ball bet is 1 in 4 (or 25 percent), the fact that I feel more confident in the ball bet implies that Iâ€™m less than 25 percent confident in self-driving cars making it to the market in a year.

Letâ€™s try decreasing the odds of winning the ball bet. Suppose the box contains sixteen balls, only one of which is gray. Now which do I prefer: betting on drawing the gray ball or betting on self-driving cars in a year?

Ball bet (1 in 16 chance of winning):

Bet on self-driving cars

Draw from a box with sixteen balls, one of which is gray. If I draw the gray ball, I get $10,000.

If fully self-driving cars are available for purchase in a year, I get $10,000.

This time, I notice that I prefer my chances on self-driving cars. After all, sometimes technological progress surprises us. Maybe one of the companies working on self-driving technology is actually farther along than theyâ€™ve been letting on. It seems unlikely, but Iâ€™d still rather gamble on that than on being lucky enough to draw the gray ball. And since the probability of drawing a gray ball is 1 in 16 (or about 6 percent), the fact that I would prefer to bet on self-driving cars implies that I am more than 6 percent confident that self-driving cars will make it to the market in a year.

Okay, letâ€™s adjust the chances of winning the ball bet back upward a little bit, to one in nine. Now which do I prefer?

Ball bet (1 in 9 chance of winning):

Bet on self-driving cars

Draw from a box with nine balls, one of which is gray. If I draw the gray ball, I get $10,000.

If fully self-driving cars are available for purchase in a year, I get $10,000.

Hmm. Iâ€™m really torn. Neither seems like a clearly better bet. The bets feel equivalent to meâ€”and since we know that the probability of winning the ball bet is 1 in 9 (or about 11 percent), that implies that I have roughly 11 percent confidence in self-driving cars coming out within a year. I still donâ€™t think the â€œSelf-driving cars will be on the market in a yearâ€ forecast is likely to come true, but Iâ€™ve gone from a glib â€œThatâ€™s crazyâ€ to a more honest best guess.

â€¢Â Â Â â€¢Â Â Â â€¢
The core skill of the previous chapter on thought experiments was a kind of self-awareness, a sense that your judgments are contingentâ€”that what seems true or reasonable or fair or desirable can change when you mentally vary some feature of the question that should have been irrelevant. The specific thought experiments we covered are all useful tools that I and other people use regularly. But the underlying shift in how you view your mindâ€™s output is even more useful.

Thereâ€™s a core skill in this chapter, too: being able to tell the difference between the feeling of making a claim and the feeling of actually trying to guess whatâ€™s true. Making a claim feels like your press secretary is speaking. It feels pat; neat and tidy. Sometimes hurried, as if youâ€™re trying to put something past yourself. The mental motion is declaring, proclaiming, insisting, or perhaps scoffing.

Trying to guess whatâ€™s true feels like being the board of directors, deciding how to bet. Thereâ€™s at least a second or two when you donâ€™t know what answer youâ€™re going to end up giving. Itâ€™s like youâ€™re squinting at the evidence, trying to summarize what you see. The mental motions involved are estimating, predicting, weighing, and deliberating.

Quantifying your uncertainty, getting calibrated, and coming up with hypothetical bets are all valuable skills in their own right. But having the self-awareness to be able to tell whether youâ€™re describing reality honestly, to the best of your abilities, is even more valuable still.

# Coping with reality

## Keeping despair at bay

One of the most fundamental human needs is to feel like things are basically okay; Thatâ€™s why most people in an emergency resort to various forms of motivated reasoning, like denial, wishful thinking, and rationalizing.

The cruel irony is that an emergency is when you most need to be clear-eyed. The more you rely on motivated reasoning, the more you degrade your ability to make judgment calls.

Commitment to finding ways of keeping despair at bay without distorting his map of reality. (â€œYou are doing the best you can. You can only do the best you can.â€)

## Honest VS. Self-deceptive ways of coping.

â€œWas it a mistake to quit my job?â€ â€œDid I offend him?â€ Someone criticizes us. We face an unpleasant choice. We fail at something. In reaction, we reach for a thought that keeps negative emotions at bayâ€”a coping strategy.

In the book Mistakes Were Made (But Not by Me), psychologists Carol Tavris and Elliot Aronson explore self-justification, a type of motivated reasoning in which you convince yourself after the fact that you made the right choice. The book is mostly about the many downsides of self-justificationâ€”how it commits us to stick with bad decisions rather than changing course, and dooms us to repeat our mistakes instead of learning from them. Still, Tavris and Aronson conclude, we need at least some amount of self-justification for the sake of our mental health: â€œWithout it, we would prolong the awful pangs of embarrassment. We would torture ourselves with regret over the road not taken or over how badly we navigated the road we did take.â€

`But is it really true that we need self-justification to prevent us from â€œtorturing ourselves with regretâ€? Couldnâ€™t we just . . . learn to not torture ourselves with regret instead?`

In Thinking, Fast and Slow, Nobel Prizeâ€“winning psychologist Daniel Kahneman points out an emotional benefit of motivated reasoning: resilience. Itâ€™s easier to bounce back from a failure if you can blame it on anyone but yourself. He uses the example of a door-to-door salesperson, a job that involves long strings of rejection: â€œWhen one has just had a door slammed in oneâ€™s face by an angry homemaker, the thought that â€˜she was an awful womanâ€™ is clearly superior to â€˜I am an inept salesperson.â€™â€

But are those really our only two options? We could instead tell ourselves, â€œYes, I screwed up that sale. But everyone makes mistakes.â€ Or â€œYes, I screwed up that sale. Still, Iâ€™m improvingâ€”I used to get doors slammed in my face every day, and now it only happens every week!â€

Surely we can find a way to bounce back from our setbacks that doesnâ€™t require us to blame them on other peopleâ€”an honest coping strategy.

> Whenever I have found out that I have blundered, or that my work has been imperfect, and when I have been contemptuously criticized, and even when I have been overpraised, so that I have felt mortified, it has been my greatest comfort to say hundreds of times to myself that â€œI have worked as hard and as well as I could, and no man can do more than this.â€

Scouts arenâ€™t invulnerable to fear, anxiety, insecurity, despair, or any of the other emotions that give rise to motivated reasoning, and they rely on coping strategies just like anyone else. They just take more care to select coping strategies that donâ€™t mess with the accuracy of their judgment.

When a negative emotion strikes, itâ€™s as if we hurriedly reach into the bucket to grab something, anything, to make ourselves feel better. We donâ€™t pay much attention to the kind of coping strategy we pull out, and whether it involves self-deception or not. As long as it makes us feel better, and itâ€™s halfway plausible, itâ€™ll do.

there is an abundance of different coping strategies, and you donâ€™t need to be so quick to go with the first thing you happen to pull out of the bucket. You can almost always find something comforting that doesnâ€™t require self-deception if you rummage around in there just a bit longer.

![Bucket of coping strategies](https://user-images.githubusercontent.com/31969517/120054182-1195a780-c04c-11eb-8093-b887d039c5c6.png)

## Make a plan

Tere are self-deceptive ways of coping with the thought of something unpleasant, such as coming up with rationalizations for why a task isnâ€™t actually necessary, or flat-out denial. But there are also honest coping strategies, like coming up with a hypothetical plan.

Itâ€™s striking how much the urge to conclude â€œThatâ€™s not trueâ€ diminishes once you feel like you have a concrete plan for what you would do if the thing were true. It doesnâ€™t have to be elaborate. Even a simple plan, like â€œHereâ€™s how I would explain the failure to my teamÂ .Â .Â .â€ or â€œHereâ€™s how I would begin my search for a new jobÂ .Â .Â .â€ goes a long way toward making you feel like you donâ€™t need to rely on denial to cope with reality.

## Notice silver linings

Conceding an argument earns me credit. It makes me more credible in other cases, because Iâ€™ve demonstrated that I donâ€™t stick to my guns just for the sake of it. Itâ€™s like Iâ€™m investing in my future ability to be convincing.

A silver lining to any mistake is the lesson youâ€™re going to extract from the experience, which you can use to help save you from similar mistakes in the future.

Remember, the goal isnâ€™t to convince yourself that your misfortune is actually a good thing. Youâ€™re not looking for a â€œsweet lemonsâ€ rationalization here. Youâ€™re recognizing a silver lining to the cloud, not trying to convince yourself the whole cloud is silver. But in many cases, thatâ€™s all you needâ€”noticing the silver lining is enough to make you willing to accept the reality of the cloud.

## Focus on a different goal

A friend of mine named Jon cofounded a software company, and in the early days, spent a lot of time recruiting and interviewing potential new hires. He soon noticed something disturbing: When he came across a talented engineer who was interested in the position, he should have felt delighted. High-quality engineers can make all the difference to the success of a new software company. But instead, Jon felt something closer to disappointment or bitterness. He would scrutinize the engineerâ€™s work, hoping to find an excuse to reject it.

Reflecting on his behavior, Jon realized: Iâ€™ve always prided myself on being the best programmer in the room. Thatâ€™s why he was motivated to denigrate his â€œcompetition,â€ as a coping strategy to protect his self-esteem.

Jon knew that his goal of needing to be the best programmer in the room was unrealistic, not to mention very counterproductive for his fledgling company. So he decided to redirect his focus and revise his goal: Rather than priding himself on being a great programmer, he decided to start priding himself on being an astute judge of programming talent. That was a satisfying enough substitute for the original goal, and actually helpful for hiring instead of counterproductive.

## Things could be worse

In his history of the AIDS crisis, How to Survive a Plague, David France profiles a small group of activists called the Treatment Action Group. They had been following the drug-testing process closely and knew that the odds of finding a miracle drug right away were slim. When the bad news about AZT broke in the summer of 1993, they were disappointedâ€”but not crushed.

Most of the activists in the Treatment Action Group were HIV-positive themselves. How had they kept up their spirits despite their realism about the chance of a cure? In part, by focusing on their gratitude for the things that could have been worse. France describes a meeting during that dispiriting summer at which one of the activists, a man named Peter Staley, said:

> Maybe that is our future, that weâ€™re gonna watch each other die. And thatâ€™s going to be awful, if thatâ€™s the case. Itâ€™s already been awful, so thereâ€™s not too much we can do about thatÂ .Â .Â . Iâ€™m justâ€”you know, I really honestly feel glad that Iâ€™ve got people to be with. Not many people have that

## Does research show that self-deceived people are happier?

Of course, the fact that the â€œself-deception causes happinessâ€ research is fatally flawed doesnâ€™t prove that self-deception canâ€™t cause happiness. It clearly can, in many cases. It just comes with the downside of eroding your judgment. And given that there are so many ways to cope that donâ€™t involve self-deception, why settle?

---

With practice, you develop your own tool kit of coping strategies that work for you. Just remember: donâ€™t settle! Your ability to see clearly is precious, and you should be reluctant to sacrifice it in exchange for emotional comfort. The good news is that you donâ€™t have to.

---

# Motivation Without Self-Deception

The self-belief model of success: If you convince yourself that you will succeed, youâ€™ll be motivated to attempt hard things and persist in the face of setbacks, such that eventually your optimism will be self-fulfilling. Conversely, if you acknowledge the long odds facing you, or contemplate the possibility of failure, youâ€™ll be too discouraged to try, and your pessimism will be a self-fulfilling prophecy as well.

> Have faith that you can successfully make it, and your feet are nerved to its accomplishment. But mistrust yourself, and think of all the sweet things you have heard the scientists say of maybes, and you will hesitate so long that, at last, all unstrung and trembling, and launching yourself in a moment of despair, you roll in the abyss. - William James

Many situations in our lives are like this, he argued. Choosing to have faith in your success, irrespective of the risk or difficulty, is the only way to summon the will to succeed. Is James right? If you could press a button and become irrationally optimistic about your chances of successâ€”should you?

## An accurate picture of your odds helps you choose between goals

To weigh all the factors successfully, you need an accurate picture of what the odds actually are.

This is the biggest problem with the self-belief approach to motivation. Because youâ€™re not supposed to think realistically about risk, it becomes impossible to ask yourself questions like, â€œIs this goal desirable enough to be worth the risk?â€ and â€œAre there any other goals that would be similarly desirable but require less risk?â€ It implicitly assumes that you donâ€™t need to make any decisions; that youâ€™ve already found the one right path, and there are no other options out there worth weighing.

Notice that in William Jamesâ€™s story of the perilous leap on the mountainâ€”his argument for the value of irrational self-beliefâ€”he has constructed the example such that thereâ€™s zero decision-making involved. You arenâ€™t given the opportunity to compare multiple options or brainstorm ideas you might have missed. The only thing you can do is try to execute the jump successfully.

In such a situation, where there is only one path available to you, maybe having a realistic picture of your odds of success on that path isnâ€™t very useful. But how often does such a situation actually occur? Even in a real-life mountain-climbing scenario, thereâ€™s never literally only one choice. Instead of attempting to leap to a nearby peak, you could try climbing down the side of the mountain. Alternately, you could stay put and hope for rescue. Whether either of those options is a better bet than jumping depends on your estimate of their relative chances of success.

And even though the rhetoric around â€œfollowing your dreamâ€ makes it sound like everyone has one and only one dream, most people have more than one thing they enjoy and are good at, or could at least become good at. Youâ€™re doing yourself a disservice if you throw yourself into the pursuit of a goal without asking: â€œIs this goal worth pursuing, compared to other things I could do instead?â€

---

At this point, you might be thinking: â€œSure, an accurate picture of the odds is important when youâ€™re choosing a path. But once youâ€™ve already made your choice, then you should switch into irrational optimism for the execution phase.â€

Of course, itâ€™s not quite as simple as â€œswitching into irrational optimism.â€ You canâ€™t just do a thoughtful, realistic calculus of risk and then erase it from your memory. But suppose you couldâ€”should you?

## An accurate picture of the odds helps you adapt your plan over time

The reality is that thereâ€™s no clear divide between the â€œdecision-makingâ€ and â€œexecutionâ€ stages of pursuing a goal. Over time, your situation will change, or youâ€™ll learn new information, and youâ€™ll need to revise your estimate of the odds.

## An accurate picture of the odds helps you decide how much to stake on success

Venture capitalist Ben Horowitz argues, in The Hard Thing About Hard Things, that thereâ€™s no point in thinking about your odds of success when building a company. â€œWhen you are building a company, you must believe there is an answer and you cannot pay attention to your odds of finding it. You just have to find it,â€ he writes. â€œIt matters not whether your chances are nine in ten or one in a thousand; your task is the same.â€

But even if your task is the same, that still leaves the question of how much you should be willing to gamble on your ability to succeed at that task. If your company has a 9 in 10 chance at success, then it might well be worth it to stake your life savings on it. If your chances are closer to 1 in 1,000, you probably want to leave that nest egg untouched.

Having an accurate picture of the odds doesnâ€™t ever stop being valuable. Still, that leaves us with a psychological challenge: If you have an accurate picture of the odds, how do you keep from getting discouraged? How do you motivate yourself to give it your all, while knowing thereâ€™s a significant chance your â€œallâ€ wonâ€™t be enough in the end?

## Bets worth taking

When Muskâ€™s friends told him that he would probably fail (SpaceX), he replied: â€œWell, I agree. I think we probably will fail.â€ In fact, he estimated that there was only about a 10 percent chance that a SpaceX craft would ever make it into orbit.

Two years later, Musk decided to invest most of the remainder of his PayPal profits into an electric car company, Tesla. That, too, he gave a roughly 10 percent chance of success.

The low odds Musk assigned to his own projectsâ€™ success left many people scratching their heads. In an appearance on 60 Minutes in 2014, interviewer Scott Pelley tried to understand Muskâ€™s logic:

Elon Musk: Well, I didnâ€™t really think Tesla would be successful. I thought we would most likely failÂ .Â .Â .

Scott Pelley: But you say you didnâ€™t expect the company to be successful? Then why try?

Elon Musk: If somethingâ€™s important enough you should try. Even if the probable outcome is failure.

Muskâ€™s low expectation of success confounds people because they assume the only reason to do something is if itâ€™s likely to succeed. But scouts arenâ€™t motivated by the thought, â€œThis is going to succeed.â€ Theyâ€™re motivated by the thought, â€œThis is a bet worth taking.â€

---

Most people are already on board with the idea of a â€œbet worth taking,â€ in at least some contexts. To give a simple example, suppose someone offered you a bet in which you roll a normal six-sided die. If it lands on a six, you win $200; if not, you lose $10. Should you take it?

Almost certainly. This is a good bet for youâ€”and you can see exactly how good it is by calculating its expected value. Thatâ€™s the average amount a bet pays out each time, if you were to take it an infinite number of times.

To calculate a betâ€™s expected value, multiply the probability of each outcome by its value and then add up those results. For this bet, the expected value would be:

`([Â¹â„â‚† probability of winning] Ã— $200) + ([âµâ„â‚† probability of losing] Ã— âˆ’$10) = $33.33 âˆ’ $8.33 = $25 `
In other words, if you took this bet many times, the average amount you would win each time is about $25. Not bad money for simply rolling a die! This is a great bet to take, even though the most likely outcome is failure.

Evaluating the probabilities involved in a real-life bet, like starting a company, is a much messier, more subjective endeavor. The possible outcomes arenâ€™t well defined the way they are in the case of the die roll. Their corresponding probabilities are subjective. And their â€œvalueâ€ involves many factors besides money: How much enjoyment would you get out of running a company? Would it leave you with useful connections and skills, even if it failed? How much of your time would it take up? How much social cachet (or stigma) would it involve?

**MUSKâ€™S THINKING ABOUT THE TESLA AND SPACEX BETS**

| **Probability**       | **Value**                                                                                                            |
| --------------------- | -------------------------------------------------------------------------------------------------------------------- |
| 10% chance of success | The company makes a big dent in one of the most pressing problems facing humanity (sustainability, space travel).    |
| 90% chance of failure | Musk loses his investment, but isnâ€™t personally ruined. The company probably makes a bit of progress on the problem. |

Overall, both Tesla and SpaceX seemed like good bets to himâ€”even though the most likely outcome for each was failure.

Another way to think about whether a bet is positive expected value is to imagine taking it many times. Would the value of the expected successes outweigh the value of the expected failures? Over the course of a lifetime, someone like Elon Musk probably has the time and money to attempt at least ten companies like Tesla and SpaceX. If his best guess is that nine of those ten companies will be failures, then the key question is: Would it be worth failing nine times in exchange for one big success?

In reality, you almost never get to repeat the exact same bet many times. But youâ€™ll have the opportunity to make many different bets over the course of your life. Youâ€™ll face bets at your company and in your career more broadly; bets on investment opportunities; chances to bet on trusting another person, or making a difficult ask, or pushing your comfort zone. And the more positive expected value bets you make, the more confident you can be that youâ€™ll end up ahead overall, even if each individual bet is far from a sure thing.

## Accepting variance gives you equanimity

Almost everyone, when asked to explain their success, gives a causal explanation like, â€œMy extra practice is finally starting to pay offâ€ or â€œItâ€™s because I believed in myself.â€ How often do you hear someone chalk their own success up to â€œrandom variationâ€?

![The psychological effect of expecting variance](https://user-images.githubusercontent.com/31969517/120054198-2c681c00-c04c-11eb-9dce-15a9e936cda3.png)

The emotional toll of variance is actually worse than this graph suggests. Weâ€™re loss averse, meaning that the pain of a loss is greater than the pleasure of a similarly sized gain. Therefore, if you donâ€™t build variance into your expectations, the low points on the spiky line graph will feel even lower than they are.

It might be motivating to believe with absolute certainty that youâ€™re going to win, but itâ€™s not realisticâ€”thereâ€™s always some element of chance involved, in any endeavor. Over time, your outcomes will fluctuate; some of your bets will turn out well, and many will turn out poorly.

But as long as you continue making positive expected value bets, that variance will mostly wash out in the long run. Building that variance into your expectations has the nice side effect of giving you equanimity. Instead of being elated when your bets pay off, and crushed when they donâ€™t, your emotions will be tied to the trend line underneath the variance.

> The goal isnâ€™t to attribute everything to luck. Itâ€™s to do your best to mentally separate out the role that luck plays in your results from the role that your decision-making plays, and to judge yourself based on the latter.

Hereâ€™s an example of Bauer doing a postmortem of his pitching in one game:

Not a great pitch, but I defend the logic behind throwing it. Walked [Jason] Castro, not a good idea. Then, tried to get [Brian] Dozier on a fastball away, came back, good pitch, but he hit it.

Notice how he gives himself credit, then blame, then creditâ€”all based on the quality of his pitching choices, independently of how they turned out.

## Coming to terms with risk

Bezos imagined being eighty years old and looking back at his life choices. Missing out on his 1994 Wall Street bonus wasnâ€™t the kind of thing he would care about decades later. But passing up the chance to participate in the growth of the internet absolutely was. â€œIf it failed, fine,â€ he decided. â€œI would be very proud of the fact when Iâ€™m 80 that I tried.â€ Thatâ€™s what clinched his decision to take the plunge, quit his job, and start the company that would become Amazon.

The â€œself-beliefâ€ model of motivation assumes that if you acknowledge the possibility of failure, then youâ€™ll be too demoralized or afraid to take risks. In that model, people who believe that failure is unthinkable are the ones who try the hardest to succeed. Yet in practice, things often seem to work the other way aroundâ€”accepting the possibility of failure in advance is liberating. It makes you bold, not timid. Itâ€™s what gives you the courage to take the risks required to achieve something big.

When one interviewer praised Elon Musk for being fearless in starting companies that other people think are crazy, Musk admitted that he actually feels fear very strongly. Heâ€™s just learned to manage that fear by coming to terms with the probability of failure. â€œSomething that can be helpful is fatalism, to some degree,â€ he explained. â€œIf you just accept the probabilities, then that diminishes fear. So in starting SpaceX, I thought the odds of success were less than 10 percent, and I just accepted that probably I would lose everything.â€

In moments when you're deciding what risks to take or stepping back to reflect on their life choices, being able to feel satisfied with the bet theyâ€™re takingâ€”even if it failsâ€”makes all the difference.

When Iâ€™m taking a bet I believe is worthwhile but risky I think about a line from a blog post I read that stayed with me: â€œYou want to get into a mental state where if the bad outcome comes to pass, you will only nod your head and say â€˜I knew this card was in the deck, and I knew the odds, and I would make the same bets again, given the same opportunities.â€™â€

---

We have a choice of coping strategies for dealing with emotions like anxiety, disappointment, regret, and fear. Some coping strategies involve self-deception, and some donâ€™tâ€”so why settle for the former?

The same logic applies to our strategies for motivating ourselves to be ambitious, take risks, and persevere when things get tough. The soldier approach to motivation requires you to believe things that arenâ€™t trueâ€”that your odds of success donâ€™t matter as long as you believe in yourself, that failure is not an option, that â€œluckâ€ is irrelevant.

Soldier morale can be effective, at least in the short term. But itâ€™s a brittle kind of morale, one that requires you to avoid or rationalize away new information that could threaten your ability to keep believing in success.

Scouts rely on a different kind of morale. Instead of being motivated by the promise of guaranteed success, a scout is motivated by the knowledge that theyâ€™re making a smart bet, which they can feel good about having made whether or not it succeeds. Even if a particular bet has a low probability of success, they know that their overall probability of success in the long run is much higher, as long as they keep making good bets. Theyâ€™re motivated by the knowledge that downturns are inevitable, but will wash out in the long run; that although failure is possible, itâ€™s also tolerable.

The scout approach to morale doesnâ€™t ask you to sacrifice your ability to make clear-eyed decisions. And itâ€™s a robust kind of morale, one that doesnâ€™t require protection from reality, because itâ€™s rooted in truth.

# Influence without overconfidence

The common wisdom is that the more confidence you can muster in your beliefs, the more influential you will be. Confidence is magnetic. It invites people to listen to you, follow you, and trust that you know what youâ€™re doing. If you look up advice on how to be influential or persuasive, youâ€™ll find lots of exhortations to **believe in yourself**

This would seem to bode poorly for scouts; if youâ€™re being intellectually honest, youâ€™re not going to have certainty about everything. Fortunately, the common wisdom isnâ€™t quite right.

## Two types of confidence

One is epistemic confidence, or certaintyâ€”how sure you are about whatâ€™s true. If you say, â€œIâ€™m 99 percent sure he is lying,â€ or â€œI guarantee this will work,â€ or â€œThereâ€™s no way the Republicans can win,â€ youâ€™re displaying a lot of epistemic confidence.

Separately, thereâ€™s social confidence, or self-assurance: Are you at ease in social situations? Do you act like you deserve to be there, like youâ€™re secure in yourself and your role in the group? Do you speak as if youâ€™re worth listening to?

![Two types of confidence](https://user-images.githubusercontent.com/31969517/120054214-3853de00-c04c-11eb-931c-636ba27e8b02.png)

We tend to conflate epistemic confidence and social confidence, treating them as if theyâ€™re a package deal. Itâ€™s easy to picture someone with both types of confidence, such as a leader pumping up his team with an inspiring pep talk about how thereâ€™s no doubt in his mind that theyâ€™re going to succeed. Itâ€™s also easy to picture someone lacking in both types of confidence, stammering nervously, â€œUh, Iâ€™m really not sure what we should do hereÂ .Â .Â .â€

But epistemic confidence and social confidence donâ€™t have to be a package deal. Just look at Benjamin Franklin. He was brimming with social confidenceâ€”famously charming, witty, and ebullient, he made friends and launched new institutions his entire life. He was basically a celebrity in France, where he was constantly surrounded by adoring women who called him â€œCher Papaâ€ (â€œDear Papaâ€).

Yet Franklin paired his abundance of social confidence with an intentional lack of epistemic confidence. It was a practice he had started when he was young, after noticing that people were more likely to reject his arguments when he used firm language like certainly and undoubtedly. So Franklin trained himself to avoid those expressions, prefacing his statements instead with caveats like â€œI thinkÂ .Â .Â .â€ or â€œIf Iâ€™m not mistakenÂ .Â .Â .â€ or â€œIt appears to me at presentÂ .Â .Â .â€

It was a tough habit to stick to at first. One of Franklinâ€™s favorite pastimes as a young man had been proving other people wrong, or what might nowadays be called â€œdestroyingâ€ people in arguments. But the habit soon got easier as he started noticing how much more receptive people were to his opinions when he expressed them gently.

## People judge you on social confidence, not epistemic confidence

When it comes to the impression you make on other people, being self-assured is more important than expressing certainty.

People sometimes bemoan the fact that â€œsuperficialâ€ things like posture and voice make such a difference in how we judge each other. But on the bright side, that means that projecting competence doesnâ€™t require self-deception. You can boost your social confidence through practice speaking up in groups, hiring a speech coach, dressing better, improving your postureâ€”all without compromising your ability to see things clearly.

The founding of Amazon is a case in point for the precedence of social confidence over epistemic confidence. The companyâ€™s big break came in the spring of 1996, when it received a visit from John Doerr, a partner at Kleiner Perkins Caufield & Byers, one of the most prestigious venture capital firms in Silicon Valley (now just Kleiner Perkins). Doerr left that meeting wowed by Amazon and ready to invest. Even better, the interest from a high-profile venture capitalist triggered a bidding war that drove Amazonâ€™s valuation up from $10 million to $60 million.

So, what exactly sold Doerr on Amazon? Iâ€™ll let him explain: â€œI walked into the door and this guy with a boisterous laugh who was just exuding energy comes bounding down the steps. In that moment, I wanted to be in business with Jeff.â€

## Two kinds of uncertainty

If a doctor says, â€œIâ€™m not sure whatâ€™s causing this,â€ itâ€™s reasonable to wonder whether a better, more experienced doctor would be able to diagnose you.

However, if the doctors sound like experts even while theyâ€™re giving an uncertain diagnosis. Theyâ€™re offering useful context and theyâ€™re giving informative estimates rather than simply saying they donâ€™t know.

![Two kinds of uncertainty](https://user-images.githubusercontent.com/31969517/120054222-473a9080-c04c-11eb-8786-35e20a432ee3.png)

When people claim that â€œadmitting uncertaintyâ€ makes you look bad, theyâ€™re invariably conflating these two very different kinds of uncertainty: uncertainty â€œin you,â€ caused by your own ignorance or lack of experience, and uncertainty â€œin the world,â€ caused by the fact that reality is messy and unpredictable. The former is often taken as a bad sign about someoneâ€™s expertise, and justifiably so. But the latter is notâ€”especially if you follow three rules for communicating uncertainty:

- ## 1. Show that uncertainty is justified

  Sometimes your audience wonâ€™t be aware of how much uncertainty exists â€œin the worldâ€ on the topic youâ€™re speaking about, and theyâ€™ll expect you to give answers with more certainty than is actually possible. Thatâ€™s okay; you just need to set their expectations.

  In fact, if you show that certainty is unrealistic, you can be more persuasive than someone who states everything with 100 percent certainty. When an attorney meets with a potential client for the first time, the client always asks how much money they can expect to be awarded. Itâ€™s tempting for the attorney to give a confident, optimistic estimate, but the reality is that he doesnâ€™t yet have enough information to go on. Instead, hereâ€™s what a prosecutor interviewed in How Leading Lawyers Think says in such a situation: â€œI tell them, â€˜Any attorney who answers that either is lying to you or does not know what heâ€™s doing, and you should run like hell.â€™â€

- ## 2. Give informed estimates

  Even if reality is messy and itâ€™s impossible to know the right answer with confidence, you can at least be confident in your analysis.

Showing that youâ€™re well-informed and well prepared on a given topic doesnâ€™t require you to overstate how much certainty is possible on that topic.

- ## 3. Have a plan

  One reason people donâ€™t like hearing uncertain answers is that it leaves them at a loss for how to act. You can reassure them by following up your uncertainty with a plan or recommendation.

Having a plan means being able to make a strong case for what you are going to do to make your business a good betâ€”a bet that you feel confident about taking, and that other people can feel confident investing in, even though success isnâ€™t guaranteed. In his 1999 CNBC interview, after acknowledging that Amazon was a risk, Jeff Bezos went on to explain why it was nevertheless a good risk to take:

Itâ€™s very, very hard to predict. But I believe that if you can focus obsessively enough on customer experience, selection, ease of use, low prices, more information to make purchase decisions with, if you can give customers all that plus great customer serviceÂ .Â .Â . I think you have a good chance. And thatâ€™s what weâ€™re trying to do.

## You don't need to promise success to be inspiring

â€œYou donâ€™t have to psych them up by lying or by being overconfident about the chance of success.â€

You can set ambitious goals. You can paint a vivid picture of the world you want to create. You can speak from the heart about why you personally care about this issue. All of those things can be inspiring, and none of them require you to make unrealistic claims.

---

you donâ€™t need to hold your opinions with 100 percent certainty in order to seem confident and competent. People simply arenâ€™t paying that much attention to how much epistemic confidence you express. Theyâ€™re paying attention to how you act, to your body language, tone, and other aspects of your social confidence, all of which are things you can cultivate without sacrificing your calibration.

Second, expressing uncertainty isnâ€™t necessarily a bad thing. It depends on whether the uncertainty is â€œin youâ€ or â€œin the world.â€ If you can demonstrate a strong command of the topic and speak with ease about your analysis and your plan, youâ€™ll seem like more of an expert, not less.

Third, you can be inspiring without overpromising. You can paint a picture of the world youâ€™re trying to create, or why your mission is important, or how your product has helped people, without claiming youâ€™re guaranteed to succeed. There are lots of ways to get people excited that donâ€™t require you to lie to others or to yourself.

whatever your goal, thereâ€™s probably a way to get it that doesnâ€™t require you to believe false things. From now on, whenever you hear someone claim that you need to self-deceive in order to be happy, motivated, or influential, you should raise a skeptical eyebrow. There are multiple paths to any goal, some of which involve self-deception and some of which donâ€™t. It may take a little more care and practice to find the latter, but in the long run, itâ€™s well worth it.

There are lots of ways to change the game board youâ€™re playing on so that you end up with better choices, instead of simply resigning yourself to picking the least-bad choice currently in front of you.

# How to Be Wrong

## Change your mind a little at a time

Youâ€™re probably already comfortable with the idea of changing your mind incrementally in some contexts. When you submit a job application, you might figure you have about a 5 percent chance of ultimately getting an offer. After they call you to set up an in-person job interview, your estimate might rise to about 10 percent. During the interview, if you feel like youâ€™re hitting it out of the park, maybe your confidence in getting an offer rises to 30 percent. If you havenâ€™t heard from them for a couple of weeks after your interview, your confidence might fall back down to 20 percent.

Whatâ€™s much rarer is for someone to do the same thing with their opinions about politics, morality, or other charged topics.

Changing your mind frequently, especially about important beliefs, might sound mentally and emotionally taxing. But, in a way, itâ€™s less stressful than the alternative. If you see the world in binary black-and-white terms, then what happens when you encounter evidence against one of your beliefs? The stakes are high: you have to find a way to dismiss the evidence, because if you canâ€™t, your entire belief is in jeopardy.

If instead you see the world in shades of gray, and you think of â€œchanging your mindâ€ as an incremental shift, then the experience of encountering evidence against one of your beliefs is very different.

## Recognizing you were wrong makes you better at being right

This is another reason superforecasters are much happier to think about what they got wrongâ€”they know that analyzing their errors is an opportunity to hone their technique. Lessons like â€œDonâ€™t assume world leaders would react the same way as youâ€ are like power-ups, upgrades to your mental arsenal that make you smarter going forward.

## Learning domain-general lessons

When a forecaster recognizes he was wrong, it helps him make better forecasts. When an investor recognizes he was wrong, it helps him make better investments.

the biggest benefits of noticing your errors: the opportunity to improve your judgment in general.

Domain-general, meaning that they apply to a wide variety of different domains, as opposed to domain-specific lessons that apply only to a single domain. Domain-general lessons are about how the world works, or how your own brain works, and about the kinds of biases that tend to influence your judgment.

> Itâ€™s easy to be fooled by cherry-picked evidence.

> If it seems like someone is saying something dumb, I might be misunderstanding them.

> Even when I feel certain, thereâ€™s still a chance Iâ€™m wrong.

ou might think these principles sound obvious and that you know them already. But â€œknowingâ€ a principle, in the sense that you read it and say, â€œYes, I know that,â€ is different from having internalized it in a way that actually changes how you think.

such knowledge doesnâ€™t really become part of you until youâ€™ve derived it for yourself by going through the experience of realizing you were wrong, asking yourself why, and seeing the effect of the bias at work.

Even when youâ€™re wrong about something random or trivial, there are still generally useful lessons to be had.

scouts think about error differently from most people. First, they revise their opinions incrementally over time, which makes it easier to be open to evidence against their beliefs. Second, they view errors as opportunities to hone their skill at getting things right, which makes the experience of realizing â€œI was wrongâ€ feel valuable, rather than just painful.

## Admitting a mistake vs Updating

An Assumption about changing your mind is that its humbling. That saying â€œI was wrongâ€ is equivalent to saying â€œI screwed upâ€â€”something you confess with contrition or sheepishness. Indeed, thatâ€™s the standard way of thinking about being wrong. Even my fellow cheerleaders for changing oneâ€™s mind tend to say things like, â€œItâ€™s okay to admit you were wrong!â€ While I appreciate the intentions behind this advice, Iâ€™m not sure it makes things much better. The word admit makes it sound like you screwed up but that you deserve to be forgiven because youâ€™re only human. It doesnâ€™t question the premise that being wrong means you screwed up.

couts reject that premise. Youâ€™ve learned new information and come to a new conclusion, but that doesnâ€™t mean you were wrong to believe differently in the past. The only reason to be contrite is if you were negligent in some way. Did you get something wrong because you followed a process you should have known was bad? Were you willfully blind or stubborn or careless?

But most of the time, being wrong doesnâ€™t mean you did something wrong. Itâ€™s not something you need to apologize for, and the appropriate attitude to have about it is neither defensive nor humbly self-flagellating, but matter-of-fact.

Instead of â€œadmitting a mistake,â€ scouts will sometimes talk about â€œupdating.â€ Thatâ€™s a reference to Bayesian updating, a technical term from probability theory for the correct way to revise a probability after learning new information. The way people use the word updating colloquially isnâ€™t nearly so precise, but it still gestures at the spirit of revising oneâ€™s beliefs in response to new evidence and arguments.

> Software engineer and product manager Devon Zuegel encourages readers to view her blog posts not as her permanent opinions, but instead as â€œa stream of thoughts, caught in the middle of updates."

If you start to think in terms of â€œupdatingâ€ instead of â€œadmitting you were wrong,â€ you may find that it takes a lot of friction out of the process. An update is routine. Low-key. Itâ€™s the opposite of an overwrought confession of sin. An update makes something better or more current without implying that its previous form was a failure.

> â€œAs Iâ€™ve gotten older, itâ€™s gotten easier to be wrong, â€œNot even to be wrong. Itâ€™s just an update: I learned this new thingÂ .Â .Â . whatâ€™s the issue?â€ - Emmett Shear

## If you're not changing your mind, you're doing something wrong

One thing that makes the Humane League unusual is their commitment to the premise that theyâ€™re always at least a little bit wrong.

Switching from one type of strategy or cause to another. In its early years, the Humane League concentrated on flashy demonstrations like picketing the homes of scientists involved in animal testing. But they found that this strategy was too alienating to be effective, and the number of animals it could save even in a best-case scenario wasnâ€™t very high. Thatâ€™s why they ultimately shifted their focus from lab animals to farm animals, and persuaded Unilever, which supplies 95 percent of the United Statesâ€™ eggs, to agree to stop killing male chicks.

Knowing that youâ€™re fallible doesnâ€™t magically prevent you from being wrong. But it does allow you to set expectations early and often, which can make it easier to accept when you are wrong. Coman-Hidy says, â€œMy intuition is that if you bring up these biases a lotâ€”that we are always going to think that weâ€™re right, that weâ€™re always thinking what weâ€™re doing is the best and most important thing to be doingÂ .Â .Â . it makes it an easier pill to swallow when inevitably something better comes along. Because youâ€™ve kind of inoculated yourself against the â€˜horrorâ€™ of having been suboptimal for some period of time.â€

Discovering you were wrong is an update, not a failure, and your worldview is a living document meant to be revised. In the next chapter weâ€™ll explore another key facet of changing your mind. Now that youâ€™ve gotten good at being wrong, itâ€™s time to get good at being confused.

# Escape your echo chamber

## How not to learn from disagreement

Learning from disagreement isn't hopelessâ€”itâ€™s that weâ€™re going about it all wrong.

- 

## Summary of key points:

-

## Further Lines of Inquiry

-

## Quotes
-

## TODO
-

## Resources Mentioned
-